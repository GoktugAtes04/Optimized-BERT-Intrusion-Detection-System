# Optimized BERT-based Intrusion Detection System

<p align="center">
  <img src="Docs/IEEE-BlackSeaCom-Logo.png" width="320" alt="IEEE BlackSeaCom 2025 Logo">
</p>

<p align="center">
  <b>IEEE BlackSeaCom 2025 Paper Publication</b><br><br>
  This repository provides the implementation and supplementary resources for the research study<br>
  <b>â€œOB-IDS: Optimized BERT-based Intrusion Detection Systemâ€</b><br>
  <i>Accepted for presentation and publication in the Proceedings of IEEE BlackSeaCom 2025</i>
</p>

---

## ğŸ“œ Citation  

If you use this work, please cite OB-IDS paper:  

**IEEE Reference:**  
G. AteÅŸ et al., "OB-IDS: Optimized BERT-based Intrusion Detection System," 2025 IEEE International Black Sea Conference on Communications and Networking (BlackSeaCom), Chisinau, Moldova, Republic of, 2025, pp. 1-4, doi: 10.1109/BlackSeaCom65655.2025.11193891. 


**BibTeX Reference:**  

```bibtex
@INPROCEEDINGS{11193891,
  author = {
    AteÅŸ, GÃ¶ktuÄŸ and 
    Ã‡elebi, BaÅŸar and 
    Semerci, Umut AytuÄŸ and 
    Ã‡apkan, Emircan and 
    YÄ±ldÄ±rÄ±m, Batuhan and 
    Ar, Ä°lktan and 
    Arsan, Taner
  },
  booktitle={2025 IEEE International Black Sea Conference on Communications and Networking (BlackSeaCom)}, 
  title={OB-IDS: Optimized BERT-based Intrusion Detection System}, 
  year={2025},
  pages={1-4},
  doi={10.1109/BlackSeaCom65655.2025.11193891}}
```

---

## ğŸ“Š Dataset Information

| ğŸ·ï¸ Feature   | ğŸ“Œ Description |
|--------------|--------------------------------------------------------------------------|
| **Name**     | ğŸ›¡ï¸ *Network Intrusion Dataset* |
| **Source**   | [![HF Dataset](https://img.shields.io/badge/HuggingFace-Network--Intrusion--Dataset-%23ffcc00?logo=huggingface&logoColor=white&style=for-the-badge)](https://huggingface.co/datasets/gates04/network-intrusion-dataset) |
| **Area of Use** | ğŸ” *Intrusion Detection* <br> ğŸš¨ *Anomaly Detection* |
| **Instances**      | ğŸ—ƒ **Total:** ~1.69 M packets <br>&nbsp;&nbsp;&nbsp;â€¢ **Train:** ~1.18 M (70%) <br>&nbsp;&nbsp;&nbsp;â€¢ **Validation:** ~0.25 M (15%) <br>&nbsp;&nbsp;&nbsp;â€¢ **Test (Inference Only):** ~0.25 M (15%) |
| **Classes** | ğŸ—‚ï¸ 24 classes <br><details><summary><small>See Full List</small></summary> <table border="1" cellspacing="0" cellpadding="5" style="font-size:90%"> <tr><th>Category</th><th>Classes</th></tr><tr><td><b>ğŸŸ¢ Normal</b></td><td>Normal</td></tr><tr><td><b>ğŸ›¡ DoS / DDoS</b></td><td>Slowloris, GoldenEye, Hulk, SlowHTTPTest, DDoS</td></tr><tr><td><b>ğŸŒ Web Attacks</b></td><td>XSS, SQL Injection, Brute Force</td></tr><tr><td><b>âš™ï¸ Other Attacks</b></td><td>Exploit, Fuzzers, Infiltration, PortScan, Bot, Heartbleed, Worms</td></tr></table> </details> | 
| **Features** | ğŸ§© Two main groups: <br>&nbsp;&nbsp;&nbsp;â€¢ **Header Fields** â€” IP src/dst, ports, protocol, flags, length, etc. <br>&nbsp;&nbsp;&nbsp;â€¢ **Payload** â€” raw packet content (hex/byte sequence) <br><details><summary><small>See TCP/IP Packet Structure </small></summary><br><img src="Docs/Fine-Tuning-Approaches/TCP-IP-Packet-Structure.png" alt="TCP/IP Packet Structure" width="90%"></details> |
| **Format**   | ğŸ—‚ï¸ *Parquet* |
| **Original Sources** | ğŸ›ï¸ CIC-IDS2017 & UNSW-NB15 datasets |

---

## ğŸ”¬ Fine-Tuning Approaches

<p id="fig-exp-structure" align="center">
  <img src="Docs/Fine-Tuning-Approaches/Experiment-Structure.png" alt="Experiment Structure â€” OB-IDS" width="500">
</p>

***1.*** ***Baseline*** ***Model:*** The Baseline model has been fine tuned for the task of classifying packets on 24 different classes by converting the flow, header and payload information contained in packets to text using BERT architecture.

***2.*** ***Quantization:*** It reduces the weights of the model using precision levels. It allows the model to consume less memory, inference time and energy in terms of the system resources at low system requirements. 

***3.*** ***Pruning:*** It aims to ensure that the model provides performance
enhancement in terms of throughput by removing the weights of the model or the components such as neurons located in its intermediate layers.

***4.*** ***Knowledge*** ***Distillation:*** It aims to use soft labels emerging from the teacher model and the ground truth labels contained in the dataset according to the loss function determined during the fine tuning process on the student model, without compromising accuracy and F1-score.

***5.*** ***Self*** ***Distillation:*** It employs the model as its own teacher and it has been shown to get better current performance by not adding parameters to the model or reducing overfitting.

---

## ğŸ“‘ Experimental Results

### Hardware Specs for Fine-Tuning & Testing

| **Setting**                  | **Fine-Tuning**                                                                  | **Testing**                                                                       |
|------------------------|----------------------------------------------------------------------------------------|--------------------------------------------------------------------------------------------|
| **GPU Model**              | NVIDIA A100                                                                            | NVIDIA L4                                                                                 |
| **Memory Capacity**          | 40 GB                                                                                  | 22.5 GB                                                                                   |
| **Memory Type**              | HBM2E (High Bandwidth Memory)                                                | GDDR6 VRAM                                                                                |
                 

### Metrics for Fine-Tuning & Testing
- **Model Size (MB)**
- **Accuracy (%)**
- **F1-score (%)**
- **Throughput (flows/s)**,which express the number of m flows in each n batch with the batch rate (n batches per second), such as (1 batch/s x 32 flows)

### Model Performance Metrics

| Method                              | Size (MB) | Accuracy (%) | F1-score (%) | Throughput (flows/s) |
|-------------------------------------|-----------|--------------|--------------|---------------------------|
| **Baseline Model**                  | 251.01    | 99.5627      | 98.70        | **3702** |
| **NF4 Quantization**                | **21.11** | 99.5639      | **98.74**    | 1504 |
| **Structured Pruning**              | 251.01    | **99.5655**  | **98.74**    | 3242 |
| **Structured + FP16**               | 84.44     | **99.5655**  | **98.74**    | 3589 |
| **NF4 (Teacher) + KD** | **72.60** | 99.19        | 94.02        | 3633 |
| **Student Model + SD**      | **72.60** | 99.23        | 96.84   | 3694 |
---

## ğŸ“Š References
